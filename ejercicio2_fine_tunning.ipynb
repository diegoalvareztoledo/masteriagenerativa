{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Explore the Hub and look for the roberta-large-mnli checkpoint. What task does it perform?\n",
        "\n",
        " Summarization\n",
        "\n",
        " **Text classification**\n",
        "\n",
        "Correct! More precisely, it classifies if two sentences are logically linked across three labels (contradiction, neutral, entailment) ‚Äî a task also called natural language inference.\n",
        "\n",
        " Text generation"
      ],
      "metadata": {
        "id": "krq4dji2gt2B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. What will the following code return?\n",
        "\n",
        " It will return classification scores for this sentence, with labels \"positive\" or \"negative\".\n",
        "\n",
        "\n",
        " It will return a generated text completing this sentence.\n",
        "\n",
        "\n",
        " **It will return the words representing persons, organizations or locations.**\n",
        "\n",
        "\n",
        "Correct! Furthermore, with grouped_entities=True, it will group together the words belonging to the same entity, like \"Hugging Face\"."
      ],
      "metadata": {
        "id": "p-fXGUJng1k_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "ner = pipeline(\"ner\", grouped_entities=True)\n",
        "\n",
        "ner(\"My name is Sylvain and I work at Hugging Face in Brooklyn.\")\n"
      ],
      "metadata": {
        "id": "GUXpqA6ThWDb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. What should replace ‚Ä¶ in this code sample?\n",
        "\n",
        "\n",
        " This <mask> has been waiting for you.\n",
        "\n",
        "\n",
        " **This [MASK] has been waiting for you.**\n",
        "\n",
        "\n",
        "Correct! Correct! This model's mask token is [MASK].\n",
        "\n",
        "\n",
        " This man has been waiting for you."
      ],
      "metadata": {
        "id": "uRyuhjhogxP4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "filler = pipeline(\"fill-mask\", model=\"bert-base-cased\")\n",
        "result = filler(\"...\")"
      ],
      "metadata": {
        "id": "7GhlP1ebhkJJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Why will this code fail?\n",
        "\n",
        "**This pipeline requires that labels be given to classify this text.**\n",
        "\n",
        "\n",
        "Correct! Right ‚Äî the correct code needs to include candidate_labels=[...].\n",
        "\n",
        "\n",
        " This pipeline requires several sentences, not just one.\n",
        "\n",
        "\n",
        " The ü§ó Transformers library is broken, as usual.\n",
        "\n",
        "\n",
        " This pipeline requires longer inputs; this one is too short."
      ],
      "metadata": {
        "id": "hIuXUy1MhpK3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "classifier = pipeline(\"zero-shot-classification\")\n",
        "result = classifier(\"This is a course about the Transformers library\")"
      ],
      "metadata": {
        "id": "FPqa5pdlh0Jf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. What does ‚Äútransfer learning‚Äù mean?\n",
        "\n",
        " Transferring the knowledge of a pretrained model to a new model by training it on the same dataset.\n",
        "\n",
        "\n",
        " **Transferring the knowledge of a pretrained model to a new model by initializing the second model with the first model's weights.**\n",
        "\n",
        "\n",
        "Correct! Correct: when the second model is trained on a new task, it *transfers* the knowledge of the first model.\n",
        "\n",
        "\n",
        " Transferring the knowledge of a pretrained model to a new model by building the second model with the same architecture as the first model."
      ],
      "metadata": {
        "id": "DSfThkfGh4Z5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6. True or false? A language model usually does not need labels for its pretraining.\n",
        "\n",
        " **True**\n",
        "\n",
        "Correct! The pretraining is usually self-supervised, which means the labels are created automatically from the inputs (like predicting the next word or filling in some masked words).\n",
        "\n",
        "\n",
        " False"
      ],
      "metadata": {
        "id": "vp4FcKOciCHA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 7. Select the sentence that best describes the terms ‚Äúmodel‚Äù, ‚Äúarchitecture‚Äù, and ‚Äúweights‚Äù.\n",
        "\n",
        " If a model is a building, its architecture is the blueprint and the weights are the people living inside.\n",
        "\n",
        "\n",
        " An architecture is a map to build a model and its weights are the cities represented on the map.\n",
        "\n",
        "\n",
        " **An architecture is a succession of mathematical functions to build a model and its weights are those functions parameters.**\n",
        "\n",
        "\n",
        "Correct! The same set of mathematical functions (architecture) can be used to build different models by using different parameters (weights)."
      ],
      "metadata": {
        "id": "ZoRtURGgiGcZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 8. Which of these types of models would you use for completing prompts with generated text?\n",
        "\n",
        "\n",
        " An encoder model\n",
        "\n",
        "\n",
        " **A decoder model**\n",
        "\n",
        "\n",
        "Correct! Decoder models are perfectly suited for text generation from a prompt.\n",
        " A sequence-to-sequence model"
      ],
      "metadata": {
        "id": "dcPTSN-oiV6p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 9. Which of those types of models would you use for summarizing texts?\n",
        "\n",
        " An encoder model\n",
        "\n",
        "\n",
        " A decoder model\n",
        "\n",
        "\n",
        " **A sequence-to-sequence model**\n",
        "\n",
        "\n",
        "Correct! Sequence-to-sequence models are perfectly suited for a summarization task."
      ],
      "metadata": {
        "id": "TH7Jr6mhidEB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 10. Which of these types of models would you use for classifying text inputs according to certain labels?\n",
        "\n",
        " **An encoder model**\n",
        "\n",
        "\n",
        "Correct! An encoder model generates a representation of the whole sentence which is perfectly suited for a task like classification.\n",
        "\n",
        "\n",
        " A decoder model\n",
        "\n",
        "\n",
        " A sequence-to-sequence model"
      ],
      "metadata": {
        "id": "MjY_Wj6Uil4X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 11. What possible source can the bias observed in a model have?\n",
        "\n",
        " **The model is a fine-tuned version of a pretrained model and it picked up its bias from it.**\n",
        "\n",
        "Correct! When applying Transfer Learning, the bias in the pretrained model used persists in the fine-tuned model.\n",
        "\n",
        " **The data the model was trained on is biased.**\n",
        "\n",
        "Correct! This is the most obvious source of bias, but not the only one.\n",
        "\n",
        " **The metric the model was optimizing for is biased.**\n",
        "\n",
        "Correct! A less obvious source of bias is the way the model is trained. Your model will blindly optimize for whatever metric you chose, without any second thoughts."
      ],
      "metadata": {
        "id": "ZHslMC8Fis_R"
      }
    }
  ]
}